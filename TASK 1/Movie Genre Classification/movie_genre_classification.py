# -*- coding: utf-8 -*-
"""MOVIE_GENRE_CLASSIFICATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R1Kljs0EhvSXGiVswG4FpjeXHvCaMDq9

**IMPORT NECESSARY LIBRARIES**
"""

# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import nltk
import subprocess

"""***LOADING THE DATSET***"""

# Filepaths
description_path = "/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/description.txt"
test_data_path = "/content/test_data.csv"
test_data_sol_path = "/content/test_data_solution.csv"
train_data_path = "/content/train_data.csv"

def read_txt_file(filepath):
    with open(filepath, 'r') as file:
        content = file.read()
    return content

train_df = pd.read_csv(train_data_path, sep=':::', header=None, engine='python')

# Give columns comprehensive names
train_df.columns = ['Id','Title','Genre','Description']

print(train_df.info())
print(type(train_df))
train_df.head()

# creating test_data dataframe

test_df = pd.read_csv(test_data_path, sep=':::', header=None, engine='python')

# Give columns comprehensive names
test_df.columns = ['Id','Title','Description']

print(test_df.info())
print(type(test_df))
test_df.head()

# creating test_data_solution dataframe

test_sol_df = pd.read_csv(test_data_sol_path, sep=':::', header=None, engine='python')

# Give columns comprehensive names
test_sol_df.columns = ['Id','Title','Genre','Description']

print(test_sol_df.info())
print(type(test_sol_df))
test_sol_df.head()

# Getting all the classes (Genres)
y_train = train_df.Genre
print(y_train)

# print(type(y_train)) # <class 'numpy.ndarray'
# print(len(y_train)) # 27

# Label encoding the output categories
le = LabelEncoder()
le.fit(y_train)
le.classes_

y_train = le.transform(y_train)
y_train

# Plotting the number of counts of each genre in the training set
train_df.Genre.value_counts()[train_df.Genre.unique()].plot(kind='bar')

# Converting the Description column (Series type) to numpy array
desc = train_df['Description']
for _ in range(2):
    print(desc[_])
    print("\n")
desc_arr = np.array(desc.tolist())
print(type(desc_arr))

def extract_features(description_array):
    """
    Extract features from the description array.

    Parameters:
        description_array: Numpy array containing all the descriptions from descriptions column in dataframe
    Returns:
        X_train: Features Matrix
        features: Feature names
    """
    wordnet = WordNetLemmatizer()
    tfidf = TfidfVectorizer(max_features=7000)

    corpus =[]

    for i,des in enumerate(description_array):
        if (i%1000==0):
            print(">>> Iteration # ",i)
        var = re.sub("[^a-zA-Z]", " ", des) # just keeping the alphabets and removing special characters and numbers
        var = var.lower() # lowercasing all
        var = var.split() # splits on space to get a list of words
        var = [wordnet.lemmatize(word) for word in var if not word in set(stopwords.words('english'))] # Lemmatizing all the words
        var = ' '.join(var)
        corpus.append(var)
    X_train = tfidf.fit_transform(corpus).toarray()
    features = tfidf.get_feature_names_out()
    return X_train,features

X_train, features = extract_features(desc_arr)

# checking the shape of X_train and y_train
print(X_train.shape)
print(type(X_train))
print(type(features))
print(features.shape)

# getting the maximum and minimum values in X_train
print("Max: ")
print(np.max(X_train))
print("Min: ")
print(np.min(X_train))

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.20,random_state=0)

print(X_train.shape)
print(X_test.shape)

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()

print(len(y_train))

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred

from sklearn.metrics import confusion_matrix
conf_mat = confusion_matrix(y_test,y_pred)
conf_mat

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test,y_pred)
print(accuracy)